// RobotBuilder Version: 2.0
//
// This file was generated by RobotBuilder. It contains sections of
// code that are automatically generated and assigned by robotbuilder.
// These sections will be updated in the future when you export to
// C++ from RobotBuilder. Do not put any code or make any change in
// the blocks indicating autogenerated code or it will be lost on an
// update. Deleting the comments indicating the section will prevent
// it from being updated in the future.


#include "VisionProcess.h"

#if 0
VisionProcess::VisionProcess() : public frc::VisionRunnerBase {

	table = NetworkTable::GetTable("GRIP/myContoursReport");
	SmartDashboard::PutBoolean(CAM_FOUNDTARGET, false);
}

// Vision processing thread

void VisionProcess::CameraVisionThread() {
	// Declare vision thread variables
	float imgWidthFloat = (float)imgWidth;

	// Our camera input source - start it up
	cs::UsbCamera camera = CameraServer::GetInstance()->StartAutomaticCapture();
	camera.SetResolution(imgWidth, imgHeight);

	// Our vision processing pipeline
	grip::GripContoursPipeline cameraPipeline;

	// Our input/output stream - available to the dashboard (uses same resolution as input)
	cs::CvSink inputStream = CameraServer::GetInstance()->GetVideo();
	cs::CvSource outputStream = CameraServer::GetInstance()->PutVideo("Contours Video", imgWidth, imgHeight);

	// OpenCV structures for processing vision frames
	cv::Mat inputFrame;
	cv::Mat processFrame;

	// Initialize dashboard settings for camera
	SmartDashboard::PutNumber(CAM_BRIGHTNESS, CAM_BRIGHTNESS_D);
	SmartDashboard::PutNumber(CAM_EXPOSURE, CAM_EXPOSURE_D);

	// Initialize dashboard settings for pipeline
	SmartDashboard::PutNumber(CAM_HUESTART, CAM_HUESTART_D);
	SmartDashboard::PutNumber(CAM_HUEEND, CAM_HUEEND_D);
	SmartDashboard::PutNumber(CAM_SATSTART, CAM_SATSTART_D);
	SmartDashboard::PutNumber(CAM_SATEND, CAM_SATEND_D);
	SmartDashboard::PutNumber(CAM_LUMSTART, CAM_LUMSTART_D);
	SmartDashboard::PutNumber(CAM_LUMEND, CAM_LUMEND_D);

	//Flag for SmartDashboard if rectContours are found
	SmartDashboard::PutBoolean(CAM_FOUNDTARGET, false);  // initialize

	// Get the current brightness and exposure from the dashboard
	camera.SetBrightness((int) SmartDashboard::GetNumber(CAM_BRIGHTNESS, CAM_BRIGHTNESS_D));
	camera.SetExposureManual((int) SmartDashboard::GetNumber(CAM_EXPOSURE, CAM_EXPOSURE_D));

	// Main loop for our vision thread
	while (true) {
		static bool pipeEnabled = false;

		// DEBUG ONLY: Wait one second before starting each processing loop
		std::this_thread::sleep_for(std::chrono::milliseconds(200));

		// Get a frame from the camera input stream
		inputStream.GrabFrame(inputFrame);

		// If the pipelineOn is false, then 'continue' will take camera out of the pipeline
		if (!SmartDashboard::GetBoolean(CAM_GEARPIPEON, false))
		{
			// If pipe was enabled, reset brightness and exposure
			if (pipeEnabled)
			{
				pipeEnabled = false;
				printf("2135: Gear Pipeline Off\n");
				// Get to a user visible brigntness and exposure
				camera.SetBrightness(100);
				camera.SetExposureAuto();
				camera.SetWhiteBalanceAuto();
			}

			//bypass pipeline
			continue;
		}
		else
		{
			// If pipe was disabled, set brightness and exposure for pipeline from dashboard
			if (!pipeEnabled)
			{
				pipeEnabled = true;
				printf("2135: Gear Pipeline On\n");
				// Get the current brightness and exposure from the dashboard
				camera.SetBrightness((int) SmartDashboard::GetNumber(CAM_BRIGHTNESS, CAM_BRIGHTNESS_D));
				camera.SetExposureManual((int) SmartDashboard::GetNumber(CAM_EXPOSURE, CAM_EXPOSURE_D));
			}
		}

		// Run vision processing pipeline generated from GRIP
		cameraPipeline.Process(inputFrame);
		// Get a reference to the pipeline output frame
		processFrame = *(cameraPipeline.gethslThresholdOutput());

		// Get a reference to the pipeline output contours
		std::vector<std::vector<cv::Point> >* filterContours = cameraPipeline.getfilterContoursOutput();

		// Declare a temporary holding list for validated rects
		std::vector<cv::Rect> validRectList;

		// Match flag initialization
		bool foundMatch = false;

		// Loop through the contours list and add bounding rects to the video stream
		for (unsigned int i = 0; i < filterContours->size(); i++) {
			// Declare an array reference to the filter contours from the pipeline
			std::vector<cv::Point>& contour = (*filterContours)[i];

			// Calculate the bounding rectangle for the contour
			cv::Rect rect = cv::boundingRect(contour);

			// Draw the rectangle formed by boundingRect on the frame bring processed -- white
			cv::rectangle(processFrame, rect, cv::Scalar(255, 255, 255));

			// Translate width and height to floating point and calculate normalized aspect ratio
			float rectRatio = ((float)rect.width / (float)rect.height) * 5.0/2.0;

			// If the rect is the correct rectangle target shape, save it in the hold list
			if ((rectRatio > 0.5) && (rectRatio < 2.0)) {
//				printf("2135: Boundary Rect in Hold List: %d\n", validRectList.size());
				printf("---> X = %d, Y = %d, W = %d, H = %d\n", rect.x, rect.y, rect.width, rect.height);
				validRectList.push_back(rect);

				// Finding the distance from the camera to the peg targets - individual rect (in)
// TEST				float pegRectDistance = VisionProcess::CalcDistToTarget((float)2.0 , imgWidthFloat, (float)rect.width);
// TEST				printf("======= Rect Distance to Peg: %3f\n", pegRectDistance);
			}
		}

		// Print the number of valid rects stored in list
//		printf("2135: Boundary rects in list: %d\n", validRectList.size());

		if (validRectList.size() == 1) {
			// Find the adjustment angle to align this rectangle to the center of the frame
			cv::Rect& rect = validRectList[0];

			float rectDistance = VisionProcess::CalcDistToTarget((float)2.0 , imgWidthFloat, (float)rect.width);
			printf("======= Rect Distance to Peg: %3f\n", rectDistance);

			float rectAngleAdjust = VisionProcess::CalcCenteringAngle(rect, imgWidthFloat, rectDistance, 2.0);
			printf("::::::::: Angle to Adjust SingleRect %3f \n", rectAngleAdjust);
			SmartDashboard::PutNumber(CAM_TURNANGLE, rectAngleAdjust);
			SmartDashboard::PutBoolean(CAM_FOUNDTARGET, true);
			// TODO: Use this distance and angle to move the robot to center the target on the frame
		}
		// Need two contours in the hold list in order to make a group
		else if (validRectList.size() > 1) {
			// Loop through validated rect list
			for (unsigned int itr = 0; itr < validRectList.size(); itr++) {
				// Initialize first rect before comparing the two
				cv::Rect& rectA = validRectList[itr];

				// Loop through remaining valid rects to validate the pair
				for (unsigned int itr2 = (itr + 1); itr < validRectList.size(); itr2++) {
					// Initialize second rect
					cv::Rect&  rectB = validRectList[itr2];

					// Don't compare rect against itself--skip it
					if (rectA == rectB) {
						continue;
					}

					// Set up the points for the bounding box around RectA and RectB
					std::vector<cv::Point> groupPoints;

					// Set up a contour with the list of points to get a bounding rect surrounding both
					groupPoints.push_back(cv::Point(rectA.x, rectA.y));
					groupPoints.push_back(cv::Point(rectA.x, rectA.y + rectA.height));
					groupPoints.push_back(cv::Point(rectA.x + rectA.width, rectA.y + rectA.height));
					groupPoints.push_back(cv::Point(rectA.x + rectA.width, rectA.y));
					groupPoints.push_back(cv::Point(rectB.x, rectB.y));
					groupPoints.push_back(cv::Point(rectB.x, rectB.y + rectB.height));
					groupPoints.push_back(cv::Point(rectB.x + rectB.width, rectB.y + rectB.height));
					groupPoints.push_back(cv::Point(rectB.x + rectB.width, rectB.y));

					// Get the boundingRect around both valid individual targets
					cv::Rect groupRect = cv::boundingRect(groupPoints);

					// Checking the groupRect height:width ratio to verify bounding Rect
					float groupRectRatio = (((float)groupRect.width / (float)groupRect.height) * (5.0 / 10.25));

					// Validate the grouped targets as being of the correct aspect ratio
					if ((groupRectRatio > 0.5) && (groupRectRatio < 2.0)) {
						// Found a possible match
		//TEST				printf("!!! ---> 2135: Found a group ratio: %3f\n", groupRectRatio);
						printf("Group ---> X = %d, Y = %d, W = %d, H = %d\n", groupRect.x, groupRect.y, groupRect.width, groupRect.height);
						// Finding the distance from the camera to the peg - group rect (in)
						float pegGroupDistance = VisionProcess::CalcDistToTarget((float)10.25 , imgWidthFloat, (float)groupRect.width);
						printf("======= Group Rect Distance to Peg: %3f\n", pegGroupDistance);

						// Add the valid group rect to the frame being processed
						cv::rectangle(processFrame, groupRect, cv::Scalar(0, 0, 255));
						foundMatch = true;

						float groupAngleAdjust = VisionProcess::CalcCenteringAngle(groupRect, imgWidthFloat, pegGroupDistance, 10.25);
						printf("::::::::: Group Angle to Adjust %3f\n", groupAngleAdjust);
						//TODO: Use this data to drive the robot
						SmartDashboard::PutNumber(CAM_TURNANGLE, groupAngleAdjust);
						SmartDashboard::PutBoolean(CAM_FOUNDTARGET, true);
		//TEST				printf("---> 2135: Group rect height: %d, width: %d, x: %d, y: %d\n", groupRect.height, groupRect.width, groupRect.x, groupRect.y);
						break;
					}
					else continue;
				} // End itr2

				if (foundMatch) {
					break;
				}
			} // End itr1
			outputStream.PutFrame(processFrame);
		}
	}
}

float VisionProcess::CalcDistToTarget(const float& rectWidthInches, const float& FOVPixels, const float& rectWidthPixels) {
	// Calculate the distance to the target given
	// RectWidthInches * FOVpixels / (2 * RectWidthPixels * tan(25 degrees)
	// Only need to change horizontal FOV angle here
	double FOVangle = 25.0;									// Using FOV horizontal angle = 25 degrees
	double angleRadian = FOVangle * 3.1415 / 180.0;			// Convert angle to radians
	return ((rectWidthInches * FOVPixels) / (2.0 * rectWidthPixels * (float)tan(angleRadian)));
}

float VisionProcess::CalcCenteringAngle(const cv::Rect& rect, const float& imgWidthScreen, const float& distToTarget, const float& RectWidthInches) {
	float pixelsToCenter;
	float rectCenterX = (float)rect.x + (float)(rect.width)/2.0;

	// Determine if the rectangle needs to go left or right to align the peg to the center of the screen
	pixelsToCenter = rectCenterX - imgWidthScreen/2.0;


	// Get the inches to center by finding out the missing values from a target width
	float inchesToCenter = RectWidthInches * pixelsToCenter / (float)(rect.width);
	// Get the radians you have to turn to get to align the peg with the center of the screen by using the inverse of tan.
	float angleToAdjustRadians = (float)atan(inchesToCenter / distToTarget);
	// Convert the radians to degrees
	float angleToAdjustDegrees = angleToAdjustRadians * 180.0 / 3.1415;
	return angleToAdjustDegrees;
}

#endif

